{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7604bd2c",
   "metadata": {},
   "source": [
    "## 언어모델\n",
    "    - 단어의 시퀀스를 보고 다음 단어에 확률을 할당하는 모델\n",
    "    - W1,...,Wn-1가 주어졌을때, n번째 단어 Wn으로 무엇이 올지를 예측하는 확률 모델\n",
    "    - *다음 단어 예측하기(An adorable little boy is spreading __)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2dc8e0",
   "metadata": {},
   "source": [
    "## 통계적 언어 모델\n",
    "    - 언어모델이란 단어 시퀀스에 대한 확률분포를 의미\n",
    "    - 언어모델은 m개 단어가 주어졌을때 m개 단어 시퀀스가 나타날 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeff94b",
   "metadata": {},
   "source": [
    "## 언어모델의 이점\n",
    "    - 언어모델은 불확실성을 단어 시퀀스의 출현 확률로 정량화가 가능하다는 장점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d247cf",
   "metadata": {},
   "source": [
    "# 신경망 언어모델(Neural Network Language Model)\n",
    "    - 통계적 언어 모델의 단점을 개선한 것이 신경망 언어모델\n",
    "    - n-gram 언어 모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 희소문제(sparsity problem). 예를들어 훈련 코퍼스에 'boy is spreading smile'라는 단어 시퀀스가 존재하지 않으면 n-gram 언어모델에서 해당 단어 시퀀스의 확률은 0이 된다.\n",
    "    \n",
    "    -결국 언어 모델 또한 단어의 의미적 유사성을 학습할 수 있도록 설계한다면, 훈련 코퍼스에 없는 단어 시퀀스에 대한 예측이라도 유사한 단어가 사용된 단어 시퀀스를 참고하여 보다 정확한 예측이 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b56e35e",
   "metadata": {},
   "source": [
    "## RNN의 문제점\n",
    "       - 1. 하나의 Weight에 입력을 계속 쌓다보니 입력이 길어질수록 이전 입력에 대한 정보가 소실되는 기울기 소실(Vanishing Gradient)문제\n",
    "       - 2. 단어별로 입력과 출력을 순환하는 RNN구조는 문장 생성엔 적합할지언정 번역에 사용하기는 어렵다는 문제가 있다.\n",
    "       - 3. 긴 의존 기간으로 인한 문제점(우리가 현재 시점의 뭔가를 얻기 위해서 멀지 않은 최근의 정보만 필요로 할 때도 있다. 예를들어 이전 단어들을 토대로 다음에 올 단어를 예측하는 언어 모델을 생각해보자. 만약 우리가 \"the coluds are in the sky\"에서의 마지막 단어를 맞추고 싶다면, 저 문장 말고는 더 볼 필요도 없다. 이경우처럼 필요한 정보를 얻기 위한 시간 격차가 크지 않다면, RNN도 지난 정보를 바탕으로 학습이 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd639028",
   "metadata": {},
   "source": [
    "# Sequence to Sequence 문제\n",
    "    - 여러개 단어를 합쳐 고정된 크기의 Weight를 Linear로 처리하는 방식은 유연성에 한계가 존재."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef2c9c",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "    - 긴 의존 기간을 필요로 하는 학습을 수행할 능력을 보유(LSTM은 긴 의존 기간의 문제를 피하기 위해 명시적으로 설계)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910fb46d",
   "metadata": {},
   "source": [
    "## LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bae6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = tf.keras.layers.LSTM(enc_units) # return_sequences 매개변수를 기본값 False로 전달\n",
    "\n",
    "  def call(self, x):\n",
    "    print(\"입력 Shape:\", x.shape)\n",
    "\n",
    "    x = self.embedding(x)\n",
    "    print(\"Embedding Layer를 거친 Shape:\", x.shape)\n",
    "\n",
    "    output = self.lstm(x)\n",
    "    print(\"LSTM Layer의 Output Shape:\", output.shape)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee24b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 30000\n",
      "Embedidng Size: 256\n",
      "LSTM Size: 512\n",
      "Batch Size: 1\n",
      "Sample Sequence Length: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "emb_size = 256\n",
    "lstm_size = 512\n",
    "batch_size = 1\n",
    "sample_seq_len = 3\n",
    "\n",
    "print(\"Vocab Size: {0}\".format(vocab_size))\n",
    "print(\"Embedidng Size: {0}\".format(emb_size))\n",
    "print(\"LSTM Size: {0}\".format(lstm_size))\n",
    "print(\"Batch Size: {0}\".format(batch_size))\n",
    "print(\"Sample Sequence Length: {0}\\n\".format(sample_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ce922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 Shape: (1, 3)\n",
      "Embedding Layer를 거친 Shape: (1, 3, 256)\n",
      "LSTM Layer의 Output Shape: (1, 512)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size, emb_size, lstm_size)\n",
    "sample_input = tf.zeros((batch_size, sample_seq_len))\n",
    "\n",
    "sample_output = encoder(sample_input)    # 컨텍스트 벡터로 사용할 인코더 LSTM의 최종 State값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08edbff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 구현에 사용된 변수들을 이어 사용함에 유의!\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = tf.keras.layers.LSTM(dec_units,\n",
    "                                     return_sequences=True) # return_sequences 매개변수를 True로 설정\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "    self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "\n",
    "  def call(self, x, context_v):  # 디코더의 입력 x와 인코더의 컨텍스트 벡터를 인자로 받는다. \n",
    "    print(\"입력 Shape:\", x.shape)\n",
    "\n",
    "    x = self.embedding(x)\n",
    "    print(\"Embedding Layer를 거친 Shape:\", x.shape)\n",
    "\n",
    "    context_v = tf.repeat(tf.expand_dims(context_v, axis=1),\n",
    "                          repeats=x.shape[1], axis=1)\n",
    "    x = tf.concat([x, context_v], axis=-1)  # 컨텍스트 벡터를 concat 해준다\n",
    "    print(\"Context Vector가 더해진 Shape:\", x.shape)\n",
    "\n",
    "    x = self.lstm(x)\n",
    "    print(\"LSTM Layer의 Output Shape:\", x.shape)\n",
    "\n",
    "    output = self.fc(x)\n",
    "    print(\"Decoder 최종 Output Shape:\", output.shape)\n",
    "\n",
    "    return self.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be7576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 30000\n",
      "Embedidng Size: 256\n",
      "LSTM Size: 512\n",
      "Batch Size: 1\n",
      "Sample Sequence Length: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab Size: {0}\".format(vocab_size))\n",
    "print(\"Embedidng Size: {0}\".format(emb_size))\n",
    "print(\"LSTM Size: {0}\".format(lstm_size))\n",
    "print(\"Batch Size: {0}\".format(batch_size))\n",
    "print(\"Sample Sequence Length: {0}\\n\".format(sample_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b61bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 Shape: (1, 3)\n",
      "Embedding Layer를 거친 Shape: (1, 3, 256)\n",
      "Context Vector가 더해진 Shape: (1, 3, 768)\n",
      "LSTM Layer의 Output Shape: (1, 3, 512)\n",
      "Decoder 최종 Output Shape: (1, 3, 30000)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size, emb_size, lstm_size)\n",
    "sample_input = tf.zeros((batch_size, sample_seq_len))\n",
    "dec_output = decoder(sample_input, sample_output)  # Decoder.call(x, context_v) 을 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070fc76",
   "metadata": {},
   "source": [
    "# seq2seq\n",
    "    - : 고정된 길이의 sequence를 입력받아, 입력 sequence에 알맞은 길이의 sequence를 출력해주는 모델\n",
    "      : encoder part와 decoder part로 구성된 모형으로 고정된 차원의 입력을 받아, 입력 값에 대응하는 가변적 길이의 결과 값을 출력해주는 모형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941a30c",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "    - encoder는 입력 sequence를 순차적으로 LSTM에 입력받아 sequence의 마지막단에서 상태를 추출.\n",
    "    \"I like apple.\"이라는 영어 문장을 \"나는 사과를 좋아합니다.\"라는 국문 문장으로 번역하는 예제를 생각해보겠습니다. 단어 단위로 Tokenization한 뒤 순차적으로 \"I\", \"like\", \"an\",\"apple\",\".\"을 입력받는다. 특히 \".\"은 입력 sequence의 종료를 나타내며 이를 별도로 End of Sentence (Eos)로 표기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250161b5",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "    - 첫 LSTM에서는 입력으로 EOS를 받는다. EOS를 받는 이유는 학습 후 모형을 운영할 때 시작값을 알수 없기 때문에 학습단계에 강제로 eos로 시작하도록 구성."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7c338",
   "metadata": {},
   "source": [
    "# Context\n",
    "    -: 마지막에 fixed-length context vector에서 4개의 단어에 대한 정보를 모두 함축.\n",
    "     문장의 길이가 길다면, context vector에서 충분히 많은 정보를 압축하기를 기대하기 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb316c",
   "metadata": {},
   "source": [
    "# Attention\n",
    "    - Attention은 주어진 쿼리(Query)에서 대해서, 키(Key)에 대한 값(Value)을 이용하여 Query의 답을 얻기 위해서 어떤 Key를 살펴봐야 되는지 계산해주는 함수. Query는 attention의 주체를 의미하고, Key는 Query에 대한 attention의 기여도를 계산할 대상, Value는 Attention 크기를 계산하기 위한 값.\n",
    "    - Q, K로 얻어진 Weight를 이용하여, Value의 Weighted sum으로 Attention Value가 출력\n",
    "    - Attention(Q,K,V) = Attention Value\n",
    "     : 어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키'와의 유사도를 구한다. 그리고 구해낸 이 유사도를 키와 맵핑되어있는 각각의 '값(Value)'에 반영. 유사도가 반영된 '값'을 모두 더해서 리턴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662beaa8",
   "metadata": {},
   "source": [
    "Q = Query : t시점의 디코더 셀에서의 은닉 상태\n",
    "K = Keys : 모든 시점의 인코더 셀의 은닉 상태들\n",
    "V = Values: 모든 시점의 인코더 셀의 은닉 상태들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4801b10d",
   "metadata": {},
   "source": [
    "# Bahdanau Attention\n",
    "    - bahdanau는 Seq2Seq2의 컨텍스트 벡터가 고정된 길이로 정보를  압축하는 것이 손실을 야기한다고 주장\n",
    "    (문장이 길어질수록 성능이 저하)\n",
    "    - bahdanau Attention은 Query를 Decoder t-1시점의 hidden state, Key, Value를 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fcb55b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden State를 100차원으로 Mapping\n",
      "\n",
      "[ H_encoder ] Shape: (1, 10, 512)\n",
      "[ W_encoder X H_encoder ] Shape: (1, 10, 100)\n",
      "\n",
      "[ H_decoder ] Shape: (1, 512)\n",
      "[ W_decoder X H_decoder ] Shape: (1, 1, 100)\n",
      "[ Score_alignment ] Shape: (1, 10, 1)\n",
      "\n",
      "최종 Weight:\n",
      " [[[0.10010065]\n",
      "  [0.12459187]\n",
      "  [0.05294646]\n",
      "  [0.07116408]\n",
      "  [0.1209837 ]\n",
      "  [0.09993205]\n",
      "  [0.10715923]\n",
      "  [0.09763453]\n",
      "  [0.10073064]\n",
      "  [0.12475681]]]\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W_decoder = tf.keras.layers.Dense(units)\n",
    "    self.W_encoder = tf.keras.layers.Dense(units)\n",
    "    self.W_combine = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, H_encoder, H_decoder):\n",
    "    print(\"[ H_encoder ] Shape:\", H_encoder.shape)\n",
    "\n",
    "    H_encoder = self.W_encoder(H_encoder)\n",
    "    print(\"[ W_encoder X H_encoder ] Shape:\", H_encoder.shape)\n",
    "\n",
    "    print(\"\\n[ H_decoder ] Shape:\", H_decoder.shape)\n",
    "    H_decoder = tf.expand_dims(H_decoder, 1)\n",
    "    H_decoder = self.W_decoder(H_decoder)\n",
    "    \n",
    "    print(\"[ W_decoder X H_decoder ] Shape:\", H_decoder.shape)\n",
    "\n",
    "    score = self.W_combine(tf.nn.tanh(H_decoder + H_encoder))\n",
    "    print(\"[ Score_alignment ] Shape:\", score.shape)\n",
    "    \n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    print(\"\\n최종 Weight:\\n\", attention_weights.numpy())\n",
    "\n",
    "    context_vector = attention_weights * H_decoder\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "W_size = 100\n",
    "\n",
    "print(\"Hidden State를 {0}차원으로 Mapping\\n\".format(W_size))\n",
    "\n",
    "attention = BahdanauAttention(W_size)\n",
    "\n",
    "enc_state = tf.random.uniform((1, 10, 512))\n",
    "dec_state = tf.random.uniform((1, 512))\n",
    "\n",
    "_ = attention(enc_state, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af8391",
   "metadata": {},
   "source": [
    "# Luong Attention 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f049b3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ H_encoder ] Shape: (1, 10, 512)\n",
      "[ W_encoder X H_encoder ] Shape: (1, 10, 512)\n",
      "[ Score_alignment ] Shape: (1, 10, 1)\n",
      "\n",
      "최종 Weight:\n",
      " [[[1.2928198e-04]\n",
      "  [3.7399300e-03]\n",
      "  [2.4067278e-05]\n",
      "  [3.0471092e-01]\n",
      "  [3.4939025e-05]\n",
      "  [3.0040616e-02]\n",
      "  [1.1312747e-01]\n",
      "  [1.7242801e-06]\n",
      "  [2.2881631e-02]\n",
      "  [5.2530938e-01]]]\n"
     ]
    }
   ],
   "source": [
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(LuongAttention, self).__init__()\n",
    "    self.W_combine = tf.keras.layers.Dense(units)\n",
    "\n",
    "  def call(self, H_encoder, H_decoder):\n",
    "    print(\"[ H_encoder ] Shape:\", H_encoder.shape)\n",
    "\n",
    "    WH = self.W_combine(H_encoder)\n",
    "    print(\"[ W_encoder X H_encoder ] Shape:\", WH.shape)\n",
    "\n",
    "    H_decoder = tf.expand_dims(H_decoder, 1)\n",
    "    alignment = tf.matmul(WH, tf.transpose(H_decoder, [0, 2, 1]))\n",
    "    print(\"[ Score_alignment ] Shape:\", alignment.shape)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(alignment, axis=1)\n",
    "    print(\"\\n최종 Weight:\\n\", attention_weights.numpy())\n",
    "\n",
    "    attention_weights = tf.squeeze(attention_weights, axis=-1)\n",
    "    context_vector = tf.matmul(attention_weights, H_encoder)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "emb_dim = 512\n",
    "\n",
    "attention = LuongAttention(emb_dim)\n",
    "\n",
    "enc_state = tf.random.uniform((1, 10, emb_dim))\n",
    "dec_state = tf.random.uniform((1, emb_dim))\n",
    "\n",
    "_ = attention(enc_state, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194f6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765ecf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63febe7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b65274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
