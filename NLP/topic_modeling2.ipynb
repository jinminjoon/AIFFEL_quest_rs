{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354809de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSA 실습\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import os\n",
    "\n",
    "csv_filename = os.getenv('HOME')+'/aiffel/topic_modelling/data/abcnews-date-text.csv'\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\", \n",
    "                           filename=csv_filename)\n",
    "\n",
    "data = pd.read_csv(csv_filename, on_bad_lines='skip')\n",
    "data\n",
    "\n",
    "text = data[['headline_text']].copy()\n",
    "text.head()\n",
    "text.nunique() # 중복을 제외하고 유일한 시퀀스를 가지는 샘플의 개수를 출력\n",
    "\n",
    "text.drop_duplicates(inplace=True) # 중복 샘플 제거\n",
    "text.reset_index(drop=True, inplace=True)\n",
    "text.shape\n",
    "\n",
    "# NLTK 토크나이저를 이용해서 토큰화\n",
    "text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n",
    "\n",
    "# 불용어 제거\n",
    "stop_words = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop_words)])\n",
    "\n",
    "text.head()\n",
    "\n",
    "# 단어 정규화. 3인칭 단수 표현 -> 1인칭 변환, 과거형 동사 -> 현재형 동사 등을 수행한다.\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "\n",
    "# 길이가 1 ~ 2인 단어는 제거.\n",
    "text = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "print(text[:5])\n",
    "\n",
    "# 역토큰화(토큰화 작업을 역으로 수행)\n",
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(text[i])\n",
    "    detokenized_doc.append(t)\n",
    "    \n",
    "train_data = detokenized_doc\n",
    "\n",
    "print(train_data)\n",
    "\n",
    "train_data[:5]\n",
    "\n",
    "# 상위 5000개의 단어만 사용\n",
    "c_vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "document_term_matrix = c_vectorizer.fit_transform(train_data)\n",
    "\n",
    "print('행렬의 크기 :',document_term_matrix.shape)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_topics = 10\n",
    "lsa_model = TruncatedSVD(n_components = n_topics)\n",
    "lsa_model.fit_transform(document_term_matrix)\n",
    "\n",
    "print(lsa_model.components_.shape)\n",
    "\n",
    "terms = c_vectorizer.get_feature_names_out() # 단어 집합. 5,000개의 단어가 저장됨.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(lsa_model.components_, terms)\n",
    "\n",
    "## 토픽모델링\n",
    "    - 문서의 집합에서 토픽을 찾아내는 프로세스\n",
    "\n",
    "## LDA\n",
    "    - LDA는 문서들이 토픽의 혼합으로 구성되어 있으며, 토픽들은 확률분포에 기반하여 단어들을 생성한다고 가정합니다. 그리고 데이터가 주어지면, LDA는 이 가정에 따라 단어들의 분포로부터 문서가 생성되는 과정을 역추적해 문서의 토픽을 찾아냄\n",
    "\n",
    "# 상위 5,000개의 단어만 사용\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tf_idf_matrix = tfidf_vectorizer.fit_transform(train_data)\n",
    "\n",
    "# TF-IDF 행렬의 크기를 확인해봅시다.\n",
    "print('행렬의 크기 :', tf_idf_matrix.shape)\n",
    "\n",
    "## scikit-learn LDA Model 활용\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', random_state=777, max_iter=1)\n",
    "lda_model.fit_transform(tf_idf_matrix)\n",
    "\n",
    "def print_top_words_as_probabilities(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTopic #{topic_idx + 1}:\")\n",
    "        \n",
    "        # 확률로 변환하기 위해 토픽의 전체 비중 합으로 나눔\n",
    "        topic_probabilities = topic / topic.sum()\n",
    "        \n",
    "        # 상위 단어 인덱스\n",
    "        top_words_idx = topic_probabilities.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        top_words_probs = topic_probabilities[top_words_idx]\n",
    "        \n",
    "        # 상위 단어와 그 확률 출력\n",
    "        for word, prob in zip(top_words, top_words_probs):\n",
    "            print(f\"{word}: {prob:.4f}\")\n",
    "\n",
    "# 각 토픽에 대해 상위 10개의 단어 확률 출력\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "n_top_words = 10\n",
    "print_top_words_as_probabilities(lda_model, terms, n_top_words)\n",
    "\n",
    "## 형태소 분석기의 필요성\n",
    "    - 한국어는 교착어(하나의 낱말이 하나의 어근과 단일한 기능을 가지는 하나 이상의 접사(affix)의 결합으로 이루어져 있는 언어), 이런 특성으로 인해 한국어는 영어와 달리 조사나 접사가 존재하며, 영어처럼 띄어쓰기 단위 토큰화가 제대로 동작하지 않는다.\n",
    "\n",
    "en_text = \"The dog ran back to the corner near the spare bedrooms\"\n",
    "print(en_text.split())\n",
    "\n",
    "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사 왔어\"\n",
    "print(kor_text.split())\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "tokenizer = Okt()\n",
    "print(tokenizer.morphs(kor_text))\n",
    "\n",
    "## 텍스트 분포를 이용한 비지도 학습 토크나이저(soynlp)\n",
    "    - soynlp는 품사 태킹, 형태소분석등을 지원하는 한국어 형태소 분석기\n",
    "    - 비지도 학습으로 형태소 분석을 한다는 특징을 갖고 있으며, 데이터에 자주 등장하는 단어들을 형태소 분석\n",
    "    - soynlp 형태소 분석기는 내부적으로 단어 점수표로 동작합니다. 이 점수는 응집 확률(cohesion probability) 과 브랜칭 엔트로피(branching entropy) 를 활용\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "txt_filename = os.getenv('HOME')+'/aiffel/topic_modelling/data/2016-10-20.txt'\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\",\\\n",
    "                            filename=txt_filename)\n",
    "\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "\n",
    "# 말뭉치에 대해서 다수의 문서로 분리\n",
    "corpus = DoublespaceLineCorpus(txt_filename)\n",
    "corpus\n",
    "\n",
    "i = 0\n",
    "for document in corpus:\n",
    "  if len(document) > 0:\n",
    "    print(document)\n",
    "    i = i+1\n",
    "  if i == 3:\n",
    "    break\n",
    "\n",
    "## soynlp는 학습과정을 거침\n",
    "![image.png](attachment:image.png)\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "from soynlp.word import WordExtractor\n",
    "\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_score_table = word_extractor.extract()\n",
    "\n",
    "word_score_table[\"반포한\"].cohesion_forward\n",
    "\n",
    "word_score_table[\"반포한강\"].cohesion_forward\n",
    "\n",
    "word_score_table[\"반포한강공\"].cohesion_forward\n",
    "\n",
    "word_score_table[\"반포한강공원\"].cohesion_forward\n",
    "\n",
    "word_score_table[\"반포한강공원에\"].cohesion_forward\n",
    "\n",
    "## 브랜칭 엔트로피\n",
    "    - 확률 분포의 엔트로피값을 사용합니다. 이는 주어진 문자열에서 다음 문자가 등장할 수 있는 가능성을 판단하는 척도\n",
    "\n",
    "word_score_table[\"디스\"].right_branching_entropy\n",
    "\n",
    "word_score_table[\"디스플\"].right_branching_entropy\n",
    "\n",
    "word_score_table[\"디스플레\"].right_branching_entropy\n",
    "\n",
    "word_score_table[\"디스플레이\"].right_branching_entropy\n",
    "\n",
    "## soynlp의 LTokenizer\n",
    "    - 한국어는 띄어쓰기 단위로 나눈 어절 토큰이 주로 L 토큰 + R 토큰의 형식을 가질 때가 많습니다. 예를 들어서 '공원에'는 '공원 + 에'로 나눌 수 있겠지요. 또는 '공부하는'은 '공부 + 하는'으로 나눌 수도 있을 것입니다. L 토크나이저는 L 토큰 + R 토큰으로 나누되, 점수가 가장 높은 L 토큰을 찾아내는 분리 기준\n",
    "\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
    "l_tokenizer = LTokenizer(scores=scores)\n",
    "l_tokenizer.tokenize(\"국제사회와 우리의 노력들로 범죄를 척결하자\", flatten=False)\n",
    "\n",
    "## 최대 점수 토크나이저\n",
    "     - 띄어쓰기가 되어 있지 않은 문장에서 점수가 높은 글자 시퀀스를 순차적으로 찾아내는 토크나이저\n",
    "\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "maxscore_tokenizer.tokenize(\"국제사회와우리의노력들로범죄를척결하자\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
