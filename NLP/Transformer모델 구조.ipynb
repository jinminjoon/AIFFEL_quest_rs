{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c21df82",
   "metadata": {},
   "source": [
    "# Transformer 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efbf401",
   "metadata": {},
   "source": [
    "### 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "796c8d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "import seaborn \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2f1c0",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "    - 해당 모델에서는 recurrence나 convolution을 전혀 사용하지 않았기 때문에, 추가적으로 위치정보 입력이 필수. 따라서 \"Positional Encoding\"을 사용해 input embedding에 위치정보를 삽입.\n",
    "    각 위치에 대해서 embedding과 동일한 dimension을 가지도록 encoding해준 뒤 그 값을 embedding값과 더해서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a18f7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8442b",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "    - 여러 개의 서브 모듈을 결합하여 완성. Embedding된 입력을 head수로 분할 split_heads(), 분할된 입력으로부터 Attention 값을 구하는 scaled_dot_product_attention(), 연산이 종료되고 분할된 Head를 다시 하나로 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03914bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "            \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "        \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "    \t\t\t\t        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAA0CAYAAACn8mCoAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAFiUAABYlAUlSJPAAABuySURBVHhe7d0HlCxFFYDhBswRc8KEOWFOGNAnoiIKCqJ4EEWFgwkUFcyCYkRRwRwRMBFMmDAhYgIVxSxmEQRFUMyJsb+y71IMMz1he/bN7Kv/nIZ93T2903Wr6saqXa9XUxUKhUKhUJh71m/+XygUCoVCYc4pSrtQKBQKhQWhKO1CoVAoFBaEorQLhUKhUFgQitIuFAqFQmFBKEq7UCgUCoUFoSjtQqFQKBQWhKK0C4VCoVBYEIrSLhQKhUJhQShKewr+9a9/VSeffHL1k5/8pDnTDf/973+r73//+9UPf/jDqmxUtzogR/IkV/ItFGaBOemkk06qfvnLX5a5Y5VTlPaE/POf/6wOOuig6vWvf3112ctetjnbDRtssEF17rnnVs94xjOq97///dV//vOf5kphESE/cnzBC16QZOsoFGbBRS960aSwn/SkJ1XHHntsUdyrmNa9x3/2s59VT3/606s//elPzZnJucMd7lC96EUvqi55yUtWX//616u99tqruTIdW221VbXnnntW6623XvWa17ym+uhHP9pcOZ8rXelK1cte9rLqRje6UXPmfEa901Oe8pTqoQ99aPOvC2ISfvOb31wdc8wx1Ste8Yrq5je/eXOlW77whS9UL3nJS6onPOEJ1UMe8pD0roXFwrD60Ic+VL3pTW+qnvvc51b3ute9miuLy4knnlg9+9nPrs4777zmzAW5/OUvX7361a+ubnCDG1THHXdctc8++zRXLsxVr3rVNH6vec1rpn8ff/zx6f5Bz37AAx5QPfOZzyzjYAT63Hvf+97q0EMPrV74whdWd73rXZsrF6bIcnFpVdp/+ctfqh/84AdJWVG4FGQI4uEPf3i1ySabpJ9zKEVK5+c//3n693Wuc53qta99bXWFK1yhOvvss6sf/ehH6Rmf+9zn0n242MUuVu28887p3hz3CS2698wzz0zn7nznO6cOcYlLXCKFp3//+9+n8x/5yEdSRwzudKc7pY57qUtdqjnzf7zTd7/73eqb3/xm9alPfar661//Wl3kIhep7nOf+6ROrpNutNFGzd0XxO848MADq8c+9rHVIx/5yJl1PO1twLCYRw2+wnyiL/Kw733veycjUR8bhLDmZz7zmep973tfddppp1Xrr79+6oOMNX3S2JgXjDVjzvd8xzvekaJOMNbue9/7Vle84hWrW97yluk7x72//vWvq3e9613pPaEv3+9+90vzAaM32sX93/jGN5Kh43Pa4Va3ulUy0m9605sOHZOLwJ///OfkRPzhD3+o/v73v6fjrLPOqp761KdWW2+9dXNXN/hdz3/+86vf/OY3Qx0XFFlOx7///e+kA376058m3aEtvO8222yTHL4VgdIeh/pL9uoO1qs9hnTUk1Jz5cLUyrZ35JFH9tasWdPbfvvte6eeempz5Xx8Pp7luZ4/jLpD9eqGSvfutttuvXPPPbe5cj4HHHDA0vPi8B18l2HUyru35ZZb9g4++ODW+1B32PQuu+yyS++cc85pzs6OX/3qV71tt902ve8f//jH5mxhESAvciM/chxGPXH3dt999zRO9ttvvzROTj/99N7++++fzrnmnnmjNr57O+2009I4a5sLfP8dd9wx3eedamO5uTKYr371q+m+UWN3kagn+t7JJ5/cq52PpXYz79QOUXNHt5x00km9LbbYorfvvvumubONIsvJ8B61I9k7/vjj0/iMdqsd0OaO2TOTnDYP9EEPelB1t7vdLVlwYcVNC2vvUY96VLIa//a3vw3N9bLgWImBMBGLaBhC9nJBctNtXrMCog9/+MPV7373u2RRbbjhhs2V2XHta187vYvIhEhDYXEgL3IjP3IcxD/+8Y8UUvzOd76Txsree++d+u81rnGN5IE555p73DtPGCt5fr6twI6X5UA94bXey2v59Kc/Xd3xjnesHvjAB84skjWK2tBK3lNX8EBFJb1XRP6ucpWrVFe72tXSz11zi1vcIkUkv/zlL6fitDZWuyxFHnj7ta5rziwP7yFaoH1FIyCVsJLRg5kVolG0wntyx8Liy4WivMc97tH6PB3qYQ972FJIyL1vf/vbk6JfDkJI8jom1Nvf/vbN2dmic9QWXHWZy1wmhfFr7625UphnyIm8TM73vOc9h05W0k21J5LuE2LMw+d+ds4197h3npCakscMTj/99OanCyOkGqkttE30FMy3v/3tqvb80u9YGzCQ1CEcdthhzZnuYPT/9re/TT9f//rX77yQNTD3Mhg5N5/4xCeWwtmDWM2yhNTTfvvt1/n8yRj4xS9+kX6mF/I2nDUzrR5nfbBClqs0AxaOnEJbJ7zyla9cPf7xj68ufvGLp3/LLX7yk59clqXFYpWPkpfx/JVC+2288capTkAevjD/kBN53fCGN7xQjUagD3/+859P3or+dPWrX725cj7OueYe9/rMokEBytXnkbZhSsEkqNJeDcBNbnKT5uzqQp45CmCNa1G+WXHjG984efLf+ta3lpTLciiyvCCMl4jGGOf9tVOzpFOlLQyRr0cVPqC0KbxJ8QzP8szA83R0xWRt8IYf/OAHN/8aHSZvg8ERns7tbne7FV22c+lLXzqFukzcX/va11ot28Lah3zIibzIjfwGIVL04x//OP0sgsQz6se5SMPou5YCzgsMYuHdQCRqENpCwWceRRgGwzjST6t1aZx0B0QE9Y9ZYq6kMM2VbSHyIsvpOOWUU5acUUV4K/menSrtj33sY0lBhlcQoZdhHaENz/AszwxMYkJKbSEcaEDV7V2EyXU+6x9ZUsM8p1kSFfqq6FmwhfmFfMgJbR4GKz0MWXUVgwa8c66BktcP5wUh/8htYlCNiTH3gQ98oNp+++1TqD8YNBd4t8MPPzzlPlcyN7iSqBgXXgYP+FrXulb6eVZwbmJJ6ve+972hdUVFlpMjahtOIL0gqrGSdKa0hU/yfAdMOvvvv/9UpfAU7BlnnNH86/9Yf02Rj7NMwr1dhMl9h3POOSdZruPmLXRmyy3uf//7p7y+9dbCpn43K/QRj3hECh3tuOOOaWlE23cSIr3c5S6XJnnLRBYNoWIWtxybZXLa5FnPelbyHC1p23XXXVMbOR7zmMekHJyJQ5uItFgu5bPayzKoN77xjUMjLT4jHGgdqOUlPuNQ52DZ4aBJhmwf/ehHL92bH867/sEPfnDgde+RF4mRDzmRV9ukbAJtS/H0YyyMii6Ni/d5y1vestQ++qB1tdHe5OH8dtttl2QxrG/mUQRtkEeBfIaxTcYK6nKjpF8puNdSUh6cvjGsBmDR0S9CaTP+RSBnTRRB+r1t/afIcjK0ZSjtWRYUDqMTpW1S+exnP5sKD7pA0YA10aeeempzZjq6CJPHd5BjHCdvQQk/+clPTh3aRiyUhdwHRWKTjZe+9KVpTfrBBx+cBsvLX/7y1ve0BpISMGhMuIuGDv2c5zwnKQEeI4XFct93332TMlQpzTK39l07WJ+uUl/7WKNuonCd8WfCOOKII5JBNChqYsKwm5zKbWukPd9mE5tttll19NFHV4973OMupIjIYI899qhe/OIXV2vWrFnyOkyqjCvXb33rW6fCIbhu/SpD1IqGPC9JaZMTeZHbMEZFigYxzWf6kdv0rqIBr3zlK9OaW4a1fQ/0TRsfaSvjxDu+7nWvGzqm89oO8sxz7nK3ZLHtttumcZOHVPuVgv0cyIRBx9BerZBfGN2Mf56rMcFA4oQccsghQ73haQmDXwSorRCryHIyRBOioFCb2G+E8UuWjBXruGdZODy10jbAfUmHUIjJdtpOp6PwiuN5PCqdWG5wOZjk+8PklMEkYXKdFp41ynLUae2UZjJ/3vOeV13vetdLxWux9E0lMOVkuQBlLS/CAu+PUORQEqFIpjFiGA+UXLTtcg8escKocVH9bsMGVaTaApQqxUlRCuEZ4GROOZL5G97whurjH/94+t4q6ClAO+uZOMAw6je+TCDa1+dZwieccEL6nMpOWzvusMMOSQYqg7V7IHesVuHud797MqxiIxvPs8OT6+SoD4na2N3PoLRrnrxk7nmEYs1lNoh8ohuXaT6To59pT7J41ateldrde9mhSptpOxENcnjnO9+ZVkuIBkxaxOR7Mrq0u+chjzrkSkHfPPLII1N48S53uUs6t1pRwxDzGQPyi1/8YjIwGaRWGTCg9K1J5qZRmK8c2nyaFTxFloPJCwrNM+95z3tSlJAszf0cTnNJl8sGc6ZW2r6krTYdJsXl5Ht5sDyXeJ5nxxq45UIh8GwjTK6gwpKc3Nsahg7JcwKPsW3pgudZXqBT6+Ch4HV8CgAsbAqbFRaKWt7nute9bvp5EDwhgwZ24JkUlvHTnva05HV2cRx11FFpkpkU7REKjkLjxebr3fWBfE0zaz2MrUBFNiiTfqVNNqxcnrG+k39Hv5vh5HdQ6BTSIDxj9913T141BW9/eQNPauVLX/pSCh1vuummS7LtJzwp8op89LxgctF/eHe5txR9inEi0qBt1XCAwTWsYCqXFa9DzhaUk7QH+Y5a0iQcz1hgUI2zLMgYI/euPdJZ4/syVAOpCWkeyk14lWNhbohir0FEn+8PSbdhHEQYvs0YWBuyXGSioBCMYDvKbb755kmWIrsMF7ISfZ4FUyttlroJzGEiEL6MEOKkCDHysuJ5cok80a7yPhb4Gyhg7QqZ9k/6g6BwowOPwuQnZyqcm3dwFllUCmsfoShK+oADDkjvyGsbN1c+bU7bIOJ1dnFQtPmkPw0MoDZDhXK92c1u1vxrfHjl8m8MC/0ph1KKUHab8UMWu+yySzLy9BFyEiZmBBiQwxQ2pjGqVgITvXdh1OT5N8akyRYMDQYk+UrhMJ7f9ra3TbRkh2Lhbd3mNrdJ0Ysg9qRGKAUhW96lyMaoSmrGs+2Ubc8pwhfRr0WBkR4RC/1y19r4CycCDE1j1NzEQMwdCjJyTm2HyNS0xagiLZMwK1mSnVoQfdHBSB5V1zNPGOOhO7QFhyh3PswxobdU7Yf+8H5qRzikopUiWub//rqtcZhaaffDo+33jJaDBmmb2CeBkmHNhqemAwuTdznJ8kqEtwgkRyeN/IcObXCa+IUmed0U4bqGjk2JDmPU9VHwKr7yla+klI10BINN+iVCWqPgAQlzgffDCPOc5RorwbBd0tqY5jOB721y9E650cEIDGVCOcdkI0/HePb/YejvUeNBsRhL2pynyIjP5ZenEIJYFiSi0taudoRjCHMKTO4RYp4GO50pShyU9olDukBUQh530PX8YNyN06fy1QLqbPq9Vm0X0TiRHZ65f1Nu2lK6KE/pdM1KydLyXbUsqtAZ19I12sZugPYpn0RxayM1L4Pkkh/+IqNCPOmsQdfjEB1VxDoKdUWhaNV95MvlwMCMPqFNtKf38n5SfmpJ5P3pIzsnqn/KlzWPQ2dKmzC7DIt4XpchRo1r4s3D5AbmrC08VhmrVcfv9/4K3SL0bZAq7FFYxTszOBhoigLHjdxQbCreY/c7A7XLwhJ9cBKjxL25Z9YVeXHUpHsQ+E4RuTCBmqAUV1kS1O+d5+9rErOiQP5vnGVBPBnpLBGP5VbpSuHJ1w9K+8RhAxFt4T0GXc8PEZhx+pRIW9scoO1CDua8mEt5YpSbospZpltWQpaUmQgn545DRrnf9ra3TStr4NokNTu+h79SNkgu+cFYl7IUCRh0PY6IKoyCE6Z9MGjM5DVKDCHvKQLr+ZS8/uK7M4aE1N1/6KGHpv4xLp0pbRhgOloXyjs6rWd2RX+YnKUTOetB5KGOadBRYycznXW5k44w2jQYXBRPFwflNUlebaXgXbPcDUCyZcFam2/ikZJo8wAGIW9lfSsMrHHW+edVuG3oB/oDWOX6ST+5xd5F3xmEynBtZXKZtCbFZBXFdr6rlISxxKPpn8j021A6+qJJymfJpi3d0DV+Fy+3P+WTH0Kdxr2JddD1/AjvtA1tw3hEpCD6kaKICIJ7QoF2SdvcsRKyZFAr1BXFUGQczpI0mL5tjMUeB+NCRwySS36EEWQeH3Q9DtfH6YuRz3Z/RG5zRHPMkxCp8h1FWh1y37FRl/koCvYYdWEIjEOnSnve0VB5mHwUhB0GCOtJJ22DtSyEF0LLd75isebFdTqtDfVVGrYpQTmRCK+PqxRyPFuI0QDs4lDBrfJ13hDOEz3BFltskbzttkHIspVzGpQfFa4SWhc2k0s0Kckr8tjbqrhjYoxc3zBMEtEHTWaDrGznXIN7fWY5yDuaMCK0JwQa+WwKu1+ZeF9VscMMFZNWhHkjHRHLgtqgDBhDPI1p+vOioZ1N5MhTEIG2i7AsD5Zj0RXmnzD82gp7V0KWni8dSImqh4qxmXv52mqeEYUIWUrd9rePucG4CcjSu0mF2oKbx5+PszD6PNcxLuuU0kZ/mHwUkVcnkLZQOityt912S6X+CkZ0fgKOUImOmlvQzr/73e9O97V5gSzwsMLzIpBx8WxhJFWgXRyiE6qM5w1WerST2or+NqUEc4/WBCG8mQ8ykAflTEnqJ4plFI1AOqX//pzIO+cyG4SJSg6NMTBstzPnXHOPeyMkGVDCvBbvPSrFw4thfCji0j89m/KOCYhRIK8ZeLZwpfD5uOPEpNRfzxHkVcywfG+aFQiLCNmEoaed+z1XIeHIaQoXT1OAOQy/2zFq34B+ZiFL4/GJT3xiyudGtBN5P6fc5plclgzd/kiLcRaeuL+Jr7IcdE78EZr8HaOgTZtG5G0cVoXS5k2ahFkro7xh5GHyUYSiNMmZ0Ich1EhoiGpCSi4m73xS9H3lOHhmvMI2eO0sWgNvXfBMpiXfSlCeLfeItfcxxxzTKj/oPwYXT8NmCQZbHp1x3TKwYRWfJsZxN8LRB1Xb+k6+Wx5tyb+ve/q9L94TBaxQyUQo7NamuBmUoRj0QwaAqER4YfpheD6eY+miPBxvq1/JBDymWIroedqr34scBCNAwdI4oeXVgLaJ+aC/LbW1jTlEVLSdzX+6SC0GIn8xd7Qp7bUlS++votr7G78U3TzD6Yp2YWT0R/JEWUXutOHOO+/cOl8bexGxVPQ5TnsHrUpbY/IsTGIKIvLlBiwma1ddc0+E8tpgUbnf53w+8FzPd80hbDlOYt5EZGtSeW8KkvdqIqMQ256RT8SjEALR4Yd5RIEJniBZntat25SAUKIj6pzywSZ8uVeDldczysKKgee+RVTa0YfIJyxLcrKGkayFksnK2umokNUfePSuGwTkHP0miH8L8VLQ/myrzg9pB6FdlbjWHGtvzwjLV7WrrTzJwzI860stb7LUUD9E3n/IPSZT3qciN7/Dd9AvAvIhJ/IatGVqjufZnYxXY7MNm/J4V4efnXPNPf0TOYMgIjjwPUIBD8KkTVnri3anM7nwpEUk9FlVwr6v9lCwxwjYc88901+iGgdr4NvCur5/LGuUchi1LGg1wSCK99Ufc+OK4aSqmPJT3T/OfDQJUdjFK8ydhjZWUpbmA+PN82w0M4niWhswumL5mzGY6xfziyJH48myLqmAYegD5j9r90UpGD7j5NODDfaxh+EQCN3aSAo2drEKTIYmYrtjyZURdlveBPIfEaqNDRzguZ7vWQ4/C89EwcMwDjnkkNRQJtX4biYv32fUMwwmitbEbzI3QQ6C1WQC1xZ29Bo2sOQrvD8FEMvJTO4EaCL3zioYVcKqIjR5jqqchbbXbhSSdcg6xSIRfYhcI89LyZKRc9pTkZfcflTQkqV3dl0buZcHHPUBoOy1qeiGjk/OFDdFQ/G5dvjhhy/J13apBhIv3CEi4xyZWTpCkeU5Nd81+g8DgNEVGLCe6/sx0mKXKJa4icgA5rmM+tvr+qDQt0nL86RLGHs8bVWvjD8Ktx+ejT7v92grh3cfNv70P2ke72jMeH9KmZeu/fXZ+N2eoVp51GTsd1I6ZMawaMt/kjcDNtp8lKE6CJ81dkQ7FD2NmmumRdszqCm52B1vuVCa8tbSGWRNOTHKpGcoQe1t/hw2cetvjET3SteMmhchFaQo0zxrGVtbZfRKyxIMGEtkvZOC41mFxqWPzBvjttsoRF61qTHD2DLOOQH2NhAZ035qf4ZFqKCtrYYgF2mrib9XLbDCCGqvpFcrzF4tmF49qJuzs6f2Unu1Bd5bs2ZNrx4ozdnCPENO5EVu5DcrzjvvvF7t0feOO+643tZbb92rjYXmyspRGza92tBp/tWO+8a9dxBnn312b6eddpr5u3qnvffeu1dPqs2ZbjjttNN6tXLubb755mku2WGHHXq1cZ9kOArv6729v3YYhzPOOCP9jq222qpXOzDN2eGspCx9tnZaenvttVevNkias7PhqKOOmqjdxsF3rg2uXm08Jllus802vQMPPLB35plnNncM55RTTultt912SafUhlVzdjLWuUK0aWAFsygt3wpvcCXgMcpHCmvGvt2F+YacyIvHHX/VaRbwylTk6o+8zmk9nuXASxo3n+m+dSWPPQgemo1F1AuIAklPSMdE1XbXSDVJoShuG2enypWSpZCy9IsUpQhX5PstHXMsAr6zdIbID1lKcdiGO9IGw5Aate+/mgFbw2qD2oBJKd1JlrsVpT0Gwp/++pGQ7LC9gbumNqiWilSEdqJzF+YbciIvoTJhRHKcFSZABZAqjmc1+a9rCHkKWw6rnl4E9AvGAaWw5ZZbphTfPCD18Na3vjUpKrugheL3faV6Rim9aZCisgPb2h4f0gFWFVmKmm+HLL2nLmaS71eU9hjIT9ghS6eS9+tyd6xhyAUbeG3LLwrzCXmRG/lFMdAsYJ1bYmIFQlsObTUgl+pgBOVL97rGZMpA32STTZozax/v672jDUahOFP+XIQw3zd8beL7WzLJK7Uywg5qsYWoGgXfNyrYu0Qth9/FgFlbMOD9iWab7NiOd82aNUvvLtpi7HZWPV44HwVRKs4V8dhDdpYeFIvUdoq8bEsH5r2qsnBByMufIiU/ciTPrmG5W36mOHE1V2PbGMjkZs9wFe4q82196Zyw4jhLPBcR7+X9vKf3jRUJsXe6dhmElReKCnluwrDz4mVzdBSbDtu/QHpnFrv+zQOMKCs8hjFozXcrtfIpjInCgYMOOigVd9SCaM52z7HHHpsKVhRRKDgqLB7kRn7kSJ5dc/TRR/f22GOP3llnndWcKazr6HOHHXZYKpCqlURztrDaWM9/Gv1dGIPYYMOSMjmKUVv9TYrlAJYP2IXLX2Vam2GdwvLgYR9xxBFp6Y2+MmxZYaGwXEzjws/2G7CUT15+krW/hcWhKO0pUDghnyik0eWfI5WvYgxYi22CL4Nu8TG8yFRYkExXe+65sHYwJ9nbwEoCedwyd6xeitIuFAqFQmFBKIVohUKhUCgsCEVpFwqFQqGwIBSlXSgUCoXCglCUdqFQKBQKC0JR2oVCoVAoLAhFaRcKhUKhsCAUpV0oFAqFwoJQlHahUCgUCgtCUdqFQqFQKCwIRWkXCoVCobAQVNX/AKUqplYSY4DgAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "id": "f0f52d97",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Network\n",
    "    - attention sub-layer에 이어서 fully connected feed-forward network 거치게 되는데 이 network는 두개의 linear transformation으로 구성되어 있고 두 transformation 사이에 ReLU 함수를 사용\n",
    "   ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec8a99c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83c59c7",
   "metadata": {},
   "source": [
    "## 레이어 수를 원하는 만큼 쌓아 실험을 자유자재로 할 수 있게 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cad2a3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerEncoderLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/1383529365.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 20개의 Encoder Layer도 한번에\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0menc_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31/1383529365.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 20개의 Encoder Layer도 한번에\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0menc_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TransformerEncoderLayer' is not defined"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "# 20개의 Linear Layer를 한번에\n",
    "linear_layers = [tf.keras.layers.Dense(30) for _ in range(N)]\n",
    "\n",
    "# 20개의 Encoder Layer도 한번에\n",
    "enc_layers = [TransformerEncoderLayer(30) for _ in range(N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c0322",
   "metadata": {},
   "source": [
    "## Encoder Layer 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97b2eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea8806e",
   "metadata": {},
   "source": [
    "## Decoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20d10398",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb88e6",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5dc9fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73830894",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3a700e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e29e63",
   "metadata": {},
   "source": [
    "## Transformer 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf60175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44544624",
   "metadata": {},
   "source": [
    "## Masking, Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b6de0",
   "metadata": {},
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bb6a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_causality_mask(tgt)\n",
    "    dec_enc_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad6fd4",
   "metadata": {},
   "source": [
    "## Attention을 할때에 PAD토큰에 attention을 주는 것을 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01b1c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13d228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79e4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
