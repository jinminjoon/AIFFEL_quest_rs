{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a76127d",
   "metadata": {},
   "source": [
    "# seq2seq으로 한글,영어 번역기 만들기 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd958347",
   "metadata": {},
   "source": [
    "### 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbc2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff349176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korean-english-park.train.tar.gz 파일이 성공적으로 다운로드되었습니다.\n",
      "korean-english-park.train.tar.gz 파일이 성공적으로 추출되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 다운로드할 파일의 URL\n",
    "url = \"https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.train.tar.gz\"\n",
    "\n",
    "# 파일을 저장할 경로\n",
    "filename = \"korean-english-park.train.tar.gz\"\n",
    "\n",
    "# 파일 다운로드\n",
    "response = requests.get(url)\n",
    "with open(filename, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "print(f\"{filename} 파일이 성공적으로 다운로드되었습니다.\")\n",
    "\n",
    "# 파일 경로\n",
    "file_name = \"korean-english-park.train.tar.gz\"\n",
    "\n",
    "# 압축 해제 및 추출\n",
    "with tarfile.open(file_name, \"r:gz\") as tar:\n",
    "    tar.extractall()  # 현재 디렉토리에 파일을 추출\n",
    "    print(f\"{file_name} 파일이 성공적으로 추출되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a01295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94123"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_path_to_file = 'korean-english-park.train.ko'\n",
    "en_path_to_file = 'korean-english-park.train.en'\n",
    "\n",
    "with open(ko_path_to_file, \"r\") as f:\n",
    "    ko_raw = f.read().splitlines()\n",
    "\n",
    "with open(en_path_to_file, \"r\") as f:\n",
    "    en_raw = f.read().splitlines()\n",
    "len(ko_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a278aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 중복 데이터 제거\n",
    "\n",
    "seen = set()  # 중복 여부를 확인할 set\n",
    "ko_clean_corp = []\n",
    "en_clean_corp = []\n",
    "\n",
    "for ko, en in zip(ko_raw, en_raw):\n",
    "    if ko not in seen:\n",
    "        ko_clean_corp.append(ko)  # 중복되지 않는 경우만 추가\n",
    "        en_clean_corp.append(en)\n",
    "        seen.add(ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c54eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77591\n",
      "77591\n",
      "그러나 박스데일은 최근 해커들이 넷스케이프 소프트웨어 보안 암호를 해독했을 때 스스로 아주 민첩함을 입증했다. 어떻게 할 것인가?\n",
      "But Barksdale proved himself to be fairly nimble recently when hackers cracked a security code in Netscape's software.\n"
     ]
    }
   ],
   "source": [
    "print(len(ko_clean_corp))\n",
    "print(len(en_clean_corp))\n",
    "\n",
    "print(ko_clean_corp[1430])\n",
    "print(en_clean_corp[1430])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f855d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 정규식을 빼고 전처리 진행\n",
    "\n",
    "def kor_preprocess_sentence(sentence):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    return sentence \n",
    "    \n",
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8ce127b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean: 하지만 멀티미디어용 컴퓨터 한 대에는 약 명의 학생들이 있다 .\n",
      "English: <start> however , there are about students for every multimedia computer . <end>\n",
      "16647\n",
      "16647\n"
     ]
    }
   ],
   "source": [
    "## 전처리 코드를 돌려서 corpus에 넣기\n",
    "\n",
    "f_enc_corpus = []\n",
    "f_kor_corpus = []\n",
    "\n",
    "\n",
    "\n",
    "for ko_sentence, en_sentence in zip(ko_clean_corp, en_clean_corp):\n",
    "    if len(kor_preprocess_sentence(ko_sentence)) < 40:\n",
    "        f_kor_corpus.append(kor_preprocess_sentence(ko_sentence))  # 한국어 문장 추가\n",
    "        f_enc_corpus.append(preprocess_sentence(en_sentence, s_token=True, e_token=True))  # 영어 문장 추가\n",
    "\n",
    "print(\"Korean:\", f_kor_corpus[125])   \n",
    "print(\"English:\", f_enc_corpus[125])   \n",
    "\n",
    "print(len(f_kor_corpus))\n",
    "print(len(f_enc_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec0b9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "\n",
    "def ko_tokenize(corpus):\n",
    "    mecab = Mecab()\n",
    "    \n",
    "    tokenized_corpus = [mecab.morphs(sentence) for sentence in corpus]\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(tokenized_corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(tokenized_corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "def en_tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91da760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 시키고 테스트, 검증셋 분리\n",
    "kor_tensor, kor_tokenizer = ko_tokenize(f_kor_corpus)\n",
    "eng_tensor, eng_tokenizer = en_tokenize(f_enc_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "943e546f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 911,  479,  430, ...,    0,    0,    0],\n",
       "       [  54,  284,    5, ...,    0,    0,    0],\n",
       "       [ 110,    5,  431, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  72,    5, 1390, ...,    0,    0,    0],\n",
       "       [  54,  304, 1147, ...,    0,    0,    0],\n",
       "       [ 895,  636,   30, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fcf3a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 단어장크기 :  15829\n",
      "영어 단어장크기 :  17604\n"
     ]
    }
   ],
   "source": [
    "print('한국어 단어장크기 : ',len(kor_tokenizer.word_index))\n",
    "print('영어 단어장크기 : ',len(eng_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34939e83",
   "metadata": {},
   "source": [
    "# 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eefeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn\n",
    "    \n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        emb_x = self.embedding(x)\n",
    "        return self.gru(emb_x)\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = BahdanauAttention(self.dec_units)   # Attention 필수 사용!\n",
    "        self.gru= tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "        \n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        out, h_dec = self.gru(out)\n",
    "\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2edfe242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input: (64, 30)\n",
      "Encoder Output: (64, 30, 1024)\n",
      "Decoder Output: (64, 17605)\n",
      "Decoder Hidden State: (64, 1024)\n",
      "Attention: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(kor_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(eng_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "print ('Encoder Input:', sample_enc.shape)\n",
    "\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63bb9b",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "131a9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            # 디코더에 dec_src = 첫번째는 start, h_dec = 첫번째는 encoder의 마지막 state\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss\n",
    "\n",
    "def test_step(src_list, kor_tokenizer, eng_tokenizer):\n",
    "    for src in src_list:\n",
    "        output = kor_preprocess_sentence(src)\n",
    "        output = mecab.morphs(output)\n",
    "        output = kor_tokenizer.texts_to_sequences(output)\n",
    "        pre_src = tf.keras.preprocessing.sequence.pad_sequences(output, padding='post')\n",
    "        pre_src = tf.reshape(pre_src, (1, pre_src.shape[0]))\n",
    "        enc_out = encoder(pre_src)\n",
    "    \n",
    "        h_dec = enc_out[:, -1]\n",
    "        bsz = 1\n",
    "        dec_src = tf.expand_dims([eng_tokenizer.word_index['<start>']] * bsz, 1)\n",
    "        \n",
    "        dec_output = []\n",
    "        \n",
    "        max_length = 25\n",
    "        for t in range(max_length):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "            index = np.argmax(pred)\n",
    "            eng_word = eng_tokenizer.index_word[index]\n",
    "            dec_output.append(eng_word)\n",
    "            if tf.equal(index, eng_tokenizer.word_index['<end>']):\n",
    "                break\n",
    "            dec_src = tf.expand_dims([index], 0)\n",
    "        \n",
    "        result = ' '.join(dec_output)\n",
    "        print('원문 : ',src)\n",
    "        print('번역 : ',result)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cb8cabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  너는 직업이 무엇이니?\n",
      "번역 :  earnings solomon listed basics slowing villagers heavens lexi unanswered absolutely stays accusations accidents surpassed angelina strangers alphabet larisa effects pardo astronomy indefinite mainland athletes recovered\n"
     ]
    }
   ],
   "source": [
    "test_step(['너는 직업이 무엇이니?'],kor_tokenizer, eng_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58e64c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 521/521 [04:25<00:00,  1.97it/s, Loss 0.3767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  오바마는 대통령이다.\n",
      "번역 :  obama will be president elect barack obama will be president elect barack obama will be president elect barack obama will be president elect barack obama\n",
      "원문 :  시민들은 도시 속에 산다.\n",
      "번역 :  louis city alderman wants people to arm themselves to arm themselves to arm themselves to arm themselves to arm themselves to arm themselves to arm\n",
      "원문 :  커피는 필요 없다.\n",
      "번역 :  the coffee has hundreds of coffee has hundreds of coffee has hundreds of coffee has hundreds of coffee has hundreds of coffee has hundreds of\n",
      "원문 :   일곱 명의 사망자가 발생했다.\n",
      "번역 :  two of the fire was taken to the children . <end>\n",
      "원문 :  진민준은 천재다.\n",
      "번역 :  a group a day checkout period . <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 521/521 [04:24<00:00,  1.97it/s, Loss 0.3326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  오바마는 대통령이다.\n",
      "번역 :  obama s speech in the first wired president . <end>\n",
      "원문 :  시민들은 도시 속에 산다.\n",
      "번역 :  india makes the gunshot wound . <end>\n",
      "원문 :  커피는 필요 없다.\n",
      "번역 :  his boss , or shoppers can buy g packs of coffee beans to make up <end>\n",
      "원문 :   일곱 명의 사망자가 발생했다.\n",
      "번역 :  his wife was taken to the hospital , and some was taken to the hospital , and some was taken to the hospital , and\n",
      "원문 :  진민준은 천재다.\n",
      "번역 :  the airstrip comes back into the spotlight back into the spotlight back into the spotlight back into the spotlight back into the spotlight back into\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 521/521 [04:24<00:00,  1.97it/s, Loss 0.2978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  오바마는 대통령이다.\n",
      "번역 :  obama s meet the first time to decelerate . <end>\n",
      "원문 :  시민들은 도시 속에 산다.\n",
      "번역 :  india cnn life are the da to arm themselves to arm themselves to arm themselves to arm themselves to arm themselves to arm themselves to\n",
      "원문 :  커피는 필요 없다.\n",
      "번역 :  ice is available , which will become compounds . <end>\n",
      "원문 :   일곱 명의 사망자가 발생했다.\n",
      "번역 :  his wife said . <end>\n",
      "원문 :  진민준은 천재다.\n",
      "번역 :  you re scheduled to a day of benjamin button . <end>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "# 예문\n",
    "src_list = ['오바마는 대통령이다.', '시민들은 도시 속에 산다.', '커피는 필요 없다.',' 일곱 명의 사망자가 발생했다.','진민준은 천재다.']\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, kor_tensor.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "    \n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(kor_tensor[idx:idx+BATCH_SIZE],\n",
    "                                eng_tensor[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                eng_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "        \n",
    "    test_step(src_list, kor_tokenizer, eng_tokenizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf8f519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2bbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4671fca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b9e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
